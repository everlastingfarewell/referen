<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reference</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Reference</h1>
        <p>
            [1] Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Sim
            ple, scalable adaptation for neural machine translation. arXiv
             preprint arXiv:1909.08478, 2019. 2
             [2] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward,
             Marcus Wainwright, Heinrich K¨ uttler, Andrew Lefrancq, Si
            mon Green, V´ıctor Vald´ es, Amir Sadik, et al. Deepmind lab.
             arXiv preprint arXiv:1612.03801, 2016. 17
             [3] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Gi
            ampiccolo. The fifth pascal recognizing textual entailment
             challenge. In TAC. Citeseer, 2009. 13
             [4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
             Food-101–mining discriminative components with random
             forests. In Computer Vision–ECCV 2014: 13th European
             Conference, Zurich, Switzerland, September 6-12, 2014,
             Proceedings, Part VI 13, pages 446–461. Springer, 2014. 16
             [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub
            biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan
            tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan
            guage models are few-shot learners. Advances in neural in
            formation processing systems, 33:1877–1901, 2020. 1, 2
             [6] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio,
             and Lucia Specia. Semeval-2017 task 1: Semantic textual
             similarity-multilingual and cross-lingual focused evaluation.
             arXiv preprint arXiv:1708.00055, 2017. 13
             [7] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
             Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapt
            ing vision transformers for scalable visual recognition. arXiv
             preprint arXiv:2205.13535, 2022. 1, 2, 5, 6
             [8] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens
            ing image scene classification: Benchmark and state of the
             art. Proceedings of the IEEE, 105(10):1865–1883, 2017. 16,
             17
             [9] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
             Mohamed, and Andrea Vedaldi. Describing textures in the
             wild. In Proceedings of the IEEE conference on computer
             vision and pattern recognition, pages 3606–3613, 2014. 16,
             17
             [10] AdamCoates, Andrew Ng, and Honglak Lee. An analysis of
             single-layer networks in unsupervised feature learning. In
             Proceedings of the fourteenth international conference on
             artificial intelligence and statistics, pages 215–223. JMLR
             Workshop and Conference Proceedings, 2011. 16
             [11] Ido Dagan, Oren Glickman, and Bernardo Magnini. The
             pascal recognising textual entailment challenge. In Machine
             Learning Challenges. Evaluating Predictive Uncertainty, Vi
            sual Object Classification, and Recognising Tectual Entail
            ment: First PASCAL Machine Learning Challenges Work
            shop, MLCW 2005, Southampton, UK, April 11-13, 2005,
             Revised Selected Papers, pages 177–190. Springer, 2006. 13
             [12] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr
             Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner,
             Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin,
             et al. Scaling vision transformers to 22 billion parameters.
             arXiv preprint arXiv:2302.05442, 2023. 1, 2
             [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
             and Li Fei-Fei. Imagenet: A large-scale hierarchical image
             database. In 2009 IEEE conference on computer vision and
             pattern recognition, pages 248–255. Ieee, 2009. 2
             [14] Li Deng. The mnist database of handwritten digit images for
             machine learning research [best of the web]. IEEE signal
             processing magazine, 29(6):141–142, 2012. 16
             [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
             Toutanova.
             Bert:
             Pre-training of deep bidirectional
             transformers for language understanding. arXiv preprint
             arXiv:1810.04805, 2018. 1, 2
             [16] Bill Dolan and Chris Brockett. Automatically constructing
             a corpus of sentential paraphrases. In Third International
             Workshop on Paraphrasing (IWP2005), 2005. 13
             [17] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming
             Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining
             Guo. Cswin transformer: A general vision transformer
             backbone with cross-shaped windows. In Proceedings of
             the IEEE/CVF Conference on Computer Vision and Pattern
             Recognition, pages 12124–12134, 2022. 2
             [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
             Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
             Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl
            vain Gelly, et al. An image is worth 16x16 words: Trans
            formers for image recognition at scale. arXiv preprint
             arXiv:2010.11929, 2020. 2
             [19] Mark Everingham, Luc Van Gool, Christopher KI Williams,
             John Winn, and Andrew Zisserman. The pascal visual object
             classes (voc) challenge. International journal of computer
             vision, 88:303–338, 2010. 16
             [20] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener
            ative visual models from few training examples: An incre
            mental bayesian approach tested on 101 object categories. In
             2004 conference on computer vision and pattern recognition
             workshop, pages 178–178. IEEE, 2004. 16, 17
             [21] Jannik Fritsch, Tobias Kuehnl, and Andreas Geiger. A new
             performance measure and evaluation benchmark for road de
            tection algorithms. In 16th International IEEE Conference
             on Intelligent Transportation Systems (ITSC 2013), pages
             1693–1700. IEEE, 2013. 16, 17
             [22] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
             William BDolan. Thethirdpascal recognizing textual entail
            ment challenge. In Proceedings of the ACL-PASCAL work
            shop on textual entailment and paraphrasing, pages 1–9,
             2007. 13
             [23] Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha
             Arnoud, and Vinay Shet. Multi-digit number recognition
             from street view imagery using deep convolutional neural
             networks. arXiv preprint arXiv:1312.6082, 2013. 16, 17
             [24] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron
             Courville, Mehdi Mirza, Ben Hamner, Will Cukierski,
             Yichuan Tang, David Thaler, Dong-Hyun Lee, et al. Chal
            lenges in representation learning: A report on three ma
            chine learning contests. In Neural Information Processing:
             9
            20th International Conference, ICONIP 2013, Daegu, Ko
            rea, November 3-7, 2013. Proceedings, Part III 20, pages
             117–124. Springer, 2013. 16
             [25] Ben Graham. Kaggle diabetic retinopathy detection compe
            tition report. University of Warwick, 22, 2015. 17
             [26] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter
            efficient transfer learning with diff pruning. arXiv preprint
             arXiv:2012.07463, 2020. 3
             [27] R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi
            ampiccolo, Bernardo Magnini, and Idan Szpektor. The sec
            ond pascal recognising textual entailment challenge. In Pro
            ceedings of the Second PASCAL Challenges Workshop on
             Recognising Textual Entailment, volume 7, 2006. 13
             [28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
             Doll´ ar, and Ross Girshick. Masked autoencoders are scalable
             vision learners. In Proceedings of the IEEE/CVF Conference
             on Computer Vision and Pattern Recognition, pages 16000
            16009, 2022. 2
             [29] Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang,
             and Xin Eric Wang. Parameter-efficient fine-tuning for vi
            sion transformers. arXiv preprint arXiv:2203.16329, 2022.
             2, 3, 5, 13
             [30] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
             Damian Borth. Eurosat: A novel dataset and deep learning
             benchmark for land use and land cover classification. IEEE
             Journal of Selected Topics in Applied Earth Observations
             and Remote Sensing, 12(7):2217–2226, 2019. 16, 17
             [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu
            sion probabilistic models. Advances in Neural Information
             Processing Systems, 33:6840–6851, 2020. 2
             [32] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
             Morrone, Quentin DeLaroussilhe, Andrea Gesmundo, Mona
             Attariyan, and Sylvain Gelly. Parameter-efficient transfer
             learning for nlp. In International Conference on Machine
             Learning, pages 2790–2799. PMLR, 2019. 1, 2, 5, 6
             [33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen
            Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
             Lora: Low-rank adaptation of large language models. arXiv
             preprint arXiv:2106.09685, 2021. 1, 2, 3, 5, 6, 7, 13
             [34] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
             Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi
            sual prompt tuning. In Computer Vision–ECCV 2022: 17th
             European Conference, Tel Aviv, Israel, October 23–27, 2022,
             Proceedings, Part XXXIII, pages 709–727. Springer, 2022. 1,
             3, 6
             [35] Shibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for
             lightweight adaptation on vision transformer. In Proceedings
             of the AAAI Conference on Artificial Intelligence, volume 37,
             pages 1060–1068, 2023. 3, 5, 6
             [36] Justin Johnson, Bharath Hariharan, Laurens Van
             Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
             Girshick. Clevr: A diagnostic dataset for compositional
             language and elementary visual reasoning. In Proceedings
             of the IEEE conference on computer vision and pattern
             recognition, pages 2901–2910, 2017. 17
             [37] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian
             Ruder. Compacter: Efficient low-rank hypercomplex adapter
             layers. Advances in Neural Information Processing Systems,
             34:1022–1035, 2021. 2, 5
             [38] Tero Karras, Samuli Laine, and Timo Aila. A style-based
             generator architecture for generative adversarial networks.
             In Proceedings of the IEEE/CVF conference on computer vi
            sion and pattern recognition, pages 4401–4410, 2019. 2
             [39] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj
             Goswami, Amanpreet Singh, Pratik Ringshia, and Davide
             Testuggine. The hateful memes challenge: Detecting hate
             speech in multimodal memes. Advances in Neural Informa
            tion Processing Systems, 33:2611–2624, 2020. 16
             [40] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
             Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White
            head, Alexander C Berg, Wan-Yen Lo, et al. Segment any
            thing. arXiv preprint arXiv:2304.02643, 2023. 1
             [41] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
             3d object representations for fine-grained categorization. In
             Proceedings of the IEEE international conference on com
            puter vision workshops, pages 554–561, 2013. 16
             [42] Alex Krizhevsky et al. Learning multiple layers of features
             from tiny images. 2009. 2, 16, 17
             [43] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning
             methods for generic object recognition with invariance to
             pose and lighting. In Proceedings of the 2004 IEEE Com
            puter Society Conference on Computer Vision and Pattern
             Recognition, 2004. CVPR 2004., volume 2, pages II–104.
             IEEE, 2004. 17
             [44] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
             of scale for parameter-efficient prompt tuning. arXiv preprint
             arXiv:2104.08691, 2021. 3
             [45] MikeLewis, YinhanLiu, NamanGoyal, MarjanGhazvinine
            jad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and
             Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
             pre-training for natural language generation, translation, and
             comprehension. arXiv preprint arXiv:1910.13461, 2019. 2
             [46] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang,
             Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng
             Liu, Yong Jae Lee, et al. Elevater: A benchmark and toolkit
             for evaluating language-augmented visual models. Advances
             in Neural Information Processing Systems, 35:9287–9301,
             2022. 5, 13
             [47] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz
            ing continuous prompts for generation. arXiv preprint
             arXiv:2101.00190, 2021. 1, 3
             [48] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao
             Wang. Scaling & shifting your features: A new baseline
             for efficient model tuning. Advances in Neural Information
             Processing Systems, 35:109–123, 2022. 1, 3, 5, 6
             [49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
             Pietro Perona, Deva Ramanan, Piotr Doll´ ar, and C Lawrence
             Zitnick. Microsoft coco: Common objects in context. In
             Computer Vision–ECCV 2014: 13th European Conference,
             Zurich, Switzerland, September 6-12, 2014, Proceedings,
             Part V 13, pages 740–755. Springer, 2014. 2
             [50] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
             Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle
            moyer, and Veselin Stoyanov. Roberta: A robustly optimized
             10
            bert pretraining approach. arXiv preprint arXiv:1907.11692,
             2019. 2, 6
             [51] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
             Zhang, Stephen Lin, and Baining Guo. Swin transformer:
             Hierarchical vision transformer using shifted windows. In
             Proceedings of the IEEE/CVF international conference on
             computer vision, pages 10012–10022, 2021. 2
             [52] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun,
             Guannan Jiang, Zhiyu Wang, and Rongrong Ji. Towards
             efficient visual adaption via structural re-parameterization.
             arXiv preprint arXiv:2302.08106, 2023. 3, 4, 5, 6
             [53] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
             Blaschko, and Andrea Vedaldi. Fine-grained visual classi
            f
             ication of aircraft. arXiv preprint arXiv:1306.5151, 2013.
             16
             [54] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander
             Lerchner. dsprites: Disentanglement testing sprites dataset,
             2017. 17
             [55] Maria-Elena Nilsback and Andrew Zisserman. Automated
             f
             lower classification over a large number of classes. In 2008
             Sixth Indian Conference on Computer Vision, Graphics &
             Image Processing, pages 722–729. IEEE, 2008. 16, 17
             [56] NamukPark and Songkuk Kim. How dovision transformers
             work? arXiv preprint arXiv:2202.06709, 2022. 4
             [57] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
             CV Jawahar. Cats and dogs. In 2012 IEEE conference on
             computer vision and pattern recognition, pages 3498–3505.
             IEEE, 2012. 16, 17
             [58] Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨ uckl´ e,
             Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non
            destructive task composition for transfer learning. arXiv
             preprint arXiv:2005.00247, 2020. 2
             [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
             Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
             AmandaAskell, Pamela Mishkin, Jack Clark, et al. Learning
             transferable visual models from natural language supervi
            sion. In International conference on machine learning, pages
             8748–8763. PMLR, 2021. 2, 16
             [60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
             Amodei, Ilya Sutskever, et al. Language models are unsu
            pervised multitask learners. OpenAI blog, 1(8):9, 2019. 2
             [61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
             Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
             Peter J Liu. Exploring the limits of transfer learning with
             a unified text-to-text transformer. The Journal of Machine
             Learning Research, 21(1):5485–5551, 2020. 1, 2
             [62] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
             Percy Liang. Squad: 100,000+ questions for machine com
            prehension of text. arXiv preprint arXiv:1606.05250, 2016.
             13
             [63] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
             Learning multiple visual domains with residual adapters. Ad
            vances in neural information processing systems, 30, 2017.
             2
             [64] Andreas R¨ uckl´ e, Gregor Geigle, Max Glockner, Tilman
             Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych.
             Adapterdrop: On the efficiency of adapters in transformers.
             arXiv preprint arXiv:2010.11918, 2020. 2, 6
             [65] Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, and
             Andrew Jackson. Fine-tuning image transformers using
             learnable memory. In Proceedings of the IEEE/CVF Con
            ference on Computer Vision and Pattern Recognition, pages
             12155–12164, 2022. 3
             [66] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
             Christopher D Manning, Andrew Y Ng, and Christopher
             Potts. Recursive deep models for semantic compositional
            ity over a sentiment treebank. In Proceedings of the 2013
             conference on empirical methods in natural language pro
            cessing, pages 1631–1642, 2013. 13
             [67] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and
             Christian Igel. The german traffic sign recognition bench
            mark: a multi-class classification competition. In The 2011
             international joint conference on neural networks, pages
             1453–1460. IEEE, 2011. 16
             [68] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter:
             Parameter-efficient transfer learning for vision-and-language
             tasks. In Proceedings of the IEEE/CVF Conference on Com
            puter Vision and Pattern Recognition, pages 5227–5237,
             2022. 2
             [69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
             Massa, Alexandre Sablayrolles, and Herv´ e J´egou. Training
             data-efficient image transformers & distillation through at
            tention. In International conference on machine learning,
             pages 10347–10357. PMLR, 2021. 2
             [70] Laurens Van der Maaten and Geoffrey Hinton. Visualiz
            ing data using t-sne. Journal of machine learning research,
             9(11), 2008. 7
             [71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko
            reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
             Polosukhin. Attention is all you need. Advances in neural
             information processing systems, 30, 2017. 2
             [72] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Co
            hen, and Max Welling. Rotation equivariant cnns for digital
             pathology. In Medical Image Computing and Computer As
            sisted Intervention–MICCAI 2018: 21st International Con
            ference, Granada, Spain, September 16-20, 2018, Proceed
            ings, Part II 11, pages 210–218. Springer, 2018. 16, 17
             [73] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
             Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
             Bowman. Superglue: A stickier benchmark for general
            purpose language understanding systems. Advances in neu
            ral information processing systems, 32, 2019. 6
             [74] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
             Omer Levy, and Samuel R Bowman. Glue: A multi-task
             benchmark and analysis platform for natural language un
            derstanding. arXiv preprint arXiv:1804.07461, 2018. 13
             [75] Peihao Wang, Wenqing Zheng, Tianlong Chen, and
             Zhangyang Wang. Anti-oversmoothing in deep vision trans
            formers via the fourier domain analysis: From theory to
             practice. arXiv preprint arXiv:2203.05962, 2022. 4
             [76] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman.
             Neural network acceptability judgments. arXiv preprint
             arXiv:1805.12471, 2018. 13
             [77] Adina Williams, Nikita Nangia, and Samuel R Bowman. A
             broad-coverage challenge corpus for sentence understanding
             11
            through inference. arXiv preprint arXiv:1704.05426, 2017.
             13
             [78] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau
            mond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim
             Rault, R´ emi Louf, Morgan Funtowicz, et al. Transformers:
             State-of-the-art natural language processing. In Proceed
            ings of the 2020 conference on empirical methods in natural
             language processing: system demonstrations, pages 38–45,
             2020. 6
             [79] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
             and Antonio Torralba. Sun database: Large-scale scene
             recognition from abbey to zoo. In 2010 IEEE computer so
            ciety conference on computer vision and pattern recognition,
             pages 3485–3492. IEEE, 2010. 16, 17
             [80] EladBenZaken, ShauliRavfogel, and YoavGoldberg. Bitfit:
             Simple parameter-efficient fine-tuning for transformer-based
             masked language-models. arXiv preprint arXiv:2106.10199,
             2021. 1, 3, 6
             [81] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov,
             Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo
            longa, Andre Susano Pinto, Maxim Neumann, Alexey Doso
            vitskiy, et al. The visual task adaptation benchmark. 2019.
             5, 13
             [82] Qingru Zhang, Minshuo Chen, Alexander Bukharin,
             Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.
             Adaptive budget allocation for parameter-efficient fine
            tuning. arXiv preprint arXiv:2303.10512, 2023. 2, 3
             [83] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural
             prompt search. arXiv preprint arXiv:2206.04673, 2022. 6
             [84] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
             Barriuso, and Antonio Torralba. Scene parsing through
             ade20k dataset. In Proceedings of the IEEE conference on
             computer vision and pattern recognition, pages 633–641,
             2017. 2
        </p>
    </header>
    <footer>
        <p>&copy;Reference</p>
    </footer>
</body>
</html>